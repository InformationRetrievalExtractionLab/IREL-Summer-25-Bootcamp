## ğŸ“š Learning & Research Resources

### ğŸ§  Prompting & APIs

- [Prompting Guide](https://www.promptingguide.ai) â€” Comprehensive guide on prompt engineering strategies and techniques.
- [Groq & Gemini API Usage (Colab)](https://colab.research.google.com/drive/1nSSBQC364cSlsrGT9va-6ogMiURFX8jX?usp=sharing) â€” Example notebook demonstrating Groq and Gemini API usage.

### ğŸ”§ Model Finetuning Notebooks

- [Text-to-Text Finetuning (Qwen2.5 7B)](<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb>) â€” Fine-tuning notebook using the Alpaca format.
- [Vision-to-Text Finetuning (LLaMA3.2 11B)](<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb>) â€” Vision-enabled LLM finetuning example.

### ğŸ‘¥ LLMs as Annotators / Evaluators

- [LLMs as Human-like Annotators](https://sites.google.com/view/lllms-as-human-like-annotators/) â€” Exploring the parallels between LLMs and human annotators.
- [LLMs for Low-Resource Language Annotation (Eval4NLP 2023)](https://aclanthology.org/2023.eval4nlp-1.8/) â€” Study on using LLMs for annotation in low-resource language settings.
- [LLM Self-Evaluation as Defense (Video)](https://www.youtube.com/watch?v=AHVwpygoGqc) â€” Presentation on how LLMs might self-evaluate to defend against attacks.
- [Great Models Think Alike (Shashwat Goel, 2024)](https://arxiv.org/abs/2502.04313) â€” Pitfalls of model agreement in benchmarking tasks.

### ğŸ›¡ï¸ Adversarial Attacks & Defenses

- [LLM-Judge Adversarial Attack (EMNLP 2024)](https://aclanthology.org/2024.emnlp-main.427.pdf) â€” Paper discussing how adversarial prompts bypass LLM judgments.
- [Survey on Jailbreak Attacks](https://arxiv.org/abs/2407.04295) â€” Overview of different jailbreak techniques and vulnerabilities.
- [Many-Shot Jailbreaking (WIRED article)](https://www.wired.com/story/ai-adversarial-attacks) â€” Real-world look at multi-shot jailbreak attacks.
- [Text Adversarial Attacks & Defenses (Wiley 2022)](https://onlinelibrary.wiley.com/doi/epdf/10.1155/2022/6458488) â€” Academic survey on textual adversarial methods.
- [English & Chinese Adversarial Attack Survey (Useful for Indic)](https://arxiv.org/abs/1902.07285) â€” Multi-lingual adversarial strategy overview.
- [Adversarial Attacks on Code Recommenders](https://www.themoonlight.io/fr/review/hallucinating-ai-hijacking-attack-large-language-models-and-malicious-code-recommenders) â€” Study on poisoning code suggestions via adversarial prompts.
- [AdversaryShield Article (Cisco)](https://outshift.cisco.com/blog/defending-llms-against-adversarial-machine-learning-attacks) â€” Conceptual defense approach against adversarial attacks.

### ğŸ­ Hallucinations in LLMs

- [Survey on Hallucinations in LLMs (2023)](https://arxiv.org/html/2311.05232) â€” Comprehensive overview of hallucination types and mitigation techniques.
