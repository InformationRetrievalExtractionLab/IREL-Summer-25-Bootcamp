## 📚 Learning & Research Resources

### 🧠 Prompting & APIs

- [Prompting Guide](https://www.promptingguide.ai) — Comprehensive guide on prompt engineering strategies and techniques.
- [Groq & Gemini API Usage (Colab)](https://colab.research.google.com/drive/1nSSBQC364cSlsrGT9va-6ogMiURFX8jX?usp=sharing) — Example notebook demonstrating Groq and Gemini API usage.

### 🔧 Model Finetuning Notebooks

- [Text-to-Text Finetuning (Qwen2.5 7B)](<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb>) — Fine-tuning notebook using the Alpaca format.
- [Vision-to-Text Finetuning (LLaMA3.2 11B)](<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb>) — Vision-enabled LLM finetuning example.

### 👥 LLMs as Annotators / Evaluators

- [LLMs as Human-like Annotators](https://sites.google.com/view/lllms-as-human-like-annotators/) — Exploring the parallels between LLMs and human annotators.
- [LLMs for Low-Resource Language Annotation (Eval4NLP 2023)](https://aclanthology.org/2023.eval4nlp-1.8/) — Study on using LLMs for annotation in low-resource language settings.
- [LLM Self-Evaluation as Defense (Video)](https://www.youtube.com/watch?v=AHVwpygoGqc) — Presentation on how LLMs might self-evaluate to defend against attacks.
- [Great Models Think Alike (Shashwat Goel, 2024)](https://arxiv.org/abs/2502.04313) — Pitfalls of model agreement in benchmarking tasks.

### 🛡️ Adversarial Attacks & Defenses

- [LLM-Judge Adversarial Attack (EMNLP 2024)](https://aclanthology.org/2024.emnlp-main.427.pdf) — Paper discussing how adversarial prompts bypass LLM judgments.
- [Survey on Jailbreak Attacks](https://arxiv.org/abs/2407.04295) — Overview of different jailbreak techniques and vulnerabilities.
- [Many-Shot Jailbreaking (WIRED article)](https://www.wired.com/story/ai-adversarial-attacks) — Real-world look at multi-shot jailbreak attacks.
- [Text Adversarial Attacks & Defenses (Wiley 2022)](https://onlinelibrary.wiley.com/doi/epdf/10.1155/2022/6458488) — Academic survey on textual adversarial methods.
- [English & Chinese Adversarial Attack Survey (Useful for Indic)](https://arxiv.org/abs/1902.07285) — Multi-lingual adversarial strategy overview.
- [Adversarial Attacks on Code Recommenders](https://www.themoonlight.io/fr/review/hallucinating-ai-hijacking-attack-large-language-models-and-malicious-code-recommenders) — Study on poisoning code suggestions via adversarial prompts.
- [AdversaryShield Article (Cisco)](https://outshift.cisco.com/blog/defending-llms-against-adversarial-machine-learning-attacks) — Conceptual defense approach against adversarial attacks.

### 🎭 Hallucinations in LLMs

- [Survey on Hallucinations in LLMs (2023)](https://arxiv.org/html/2311.05232) — Comprehensive overview of hallucination types and mitigation techniques.
